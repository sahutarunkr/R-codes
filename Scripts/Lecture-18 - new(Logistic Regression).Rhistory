# We'll look at German Credit History

myData = germancredit
summary(myData)

# Note we can always re-label the factors
levels(myData$history)=c("good","good","poor","poor","terrible")
summary(myData$history)

# There are 1000 observations, we will use 900 to train the model
train = sample(1000,900)
lr.model = glm(Default~., data=myData, subset=train, family="binomial")
summary(lr.model)

# Not a great reduction in deviance ... let's test the model
lr.pred = predict(lr.model,newdata = myData[-train,-1], type="response")
pred = rep(0,100)
pred[lr.pred > .5] = 1
table(myData[-train,]$Default,pred)

# If our goal is to predict default, the model is not good
# Let's try to do PCA on the predictors ...
# We need to convert the predictors into a separate DF
myData2 = model.matrix(Default~. -1, data = myData)
head(myData2)
pcaCredit = prcomp(myData2, scale. = TRUE)
head(pcaCredit$x)
pcaVar = pcaCredit$sdev^2
pve=pcaVar/sum(pcaVar)
plot(pve)
plot(cumsum (pve ), xlab=" Principal Component ", ylab ="
Cumulative Proportion of Variance Explained ", ylim=c(0,1) ,
type='b')
# Bad news = PCA does not seem to help much in this situation

# Let's look at another model:
library(readr)
hyper <- read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/hypothyroid.data",
col_names = FALSE)
View(hyper)

# Note we loaded directly from URL, and note we assigned
# a name to the DF
library(readr)
names <- read_delim("https://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/hypothyroid.names",
"\t", escape_double = FALSE, trim_ws = TRUE, col_names = FALSE)
View(names)

# We remove the colons and apply to column names of hyper
names2 <- gsub(pattern =":|[.]",x = names[[1]], replacement="")
names2
names2[1]="target"
names2
colnames(hyper) <- names2
View(hyper)

# Unfortunately, we did not change the '?' to 'NA' when we
# read in the data. We can fix it now ...
hyper[ hyper == "?" ] = NA
hyper = as.data.frame(unclass(hyper))
summary(hyper)

# TBG looks useless, and several numerics are now reporting
# as factors. Clean up
myData = hyper
myData$TBG=NULL
myData$TBG_measured=NULL
myData$age = as.numeric(myData$age)
myData$TSH = as.numeric(myData$TSH)
myData$T3 = as.numeric(myData$T3)
myData$TT4 = as.numeric(myData$TT4)
myData$T4U = as.numeric(myData$T4U)
myData$FTI = as.numeric(myData$FTI)
summary(myData)
myData_2 = na.omit(myData)
summary(myData_2)

# The 'measure' variables are now useless ...
myData_2$TSH_measured = NULL
myData_2$T3_measured = NULL
myData_2$TT4_measured = NULL
myData_2$T4U_measured = NULL
myData_2$FTI_measured = NULL
myData_2$target <- ifelse(myData_2$target=='negative',0,1)
dim(myData_2)
print(table(myData_2$target))

# This is an unbalanced dataset!
# We'll do a simple CV to start. 
train = sample(2000,1500)
lr.model = glm(target~., data=myData_2, subset=train, family="binomial")
summary(lr.model)
plot(myData_2$TSH,myData_2$target)
lr.pred = predict(lr.model,newdata = myData_2[-train,-1], type="response")
pred = rep(0,500)
pred[lr.pred > .5] = 1
table(myData_2[-train,]$target,pred)

# Precision is 65%, recall is only 37%
mean(myData_2[-train,]$target == pred)

# 94% accuracy! Of course, could just predict all '0' ...
summary(myData_2$target)

# Let's change the threshold ...
pred = rep(0,500)
pred[lr.pred > .25] = 1
table(myData_2[-train,]$target,pred)

# Not much help. We will try SMOTE. Attach the DMwR package.
?SMOTE
myData_2$target=as.factor(myData_2$target)
myData_3 <- SMOTE(target ~ ., myData_2[train,], perc.over = 100, perc.under=200)
myData_3$target = as.numeric(myData_3$target)
dim(myData_2[train,])
dim(myData_3)
summary(myData_3$target)
myData_3$target = myData_3$target-1
summary(myData_3$target)

# Now we train the model using the new training set ...
lr.model = glm(target~., data=myData_3, family="binomial")
lr.pred = predict(lr.model,newdata = myData_2[-train,-1], type="response")
pred = rep(0,500)
pred[lr.pred > .5] = 1
table(myData_2[-train,]$target,pred)
mean(myData_2[-train,]$target == pred)

# Better recall, worse precision. Need to be careful with
# parameter selection. 


