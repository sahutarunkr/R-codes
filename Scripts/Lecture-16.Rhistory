# First we load the student performance data, which
# we will use to compare the methods.
myData = student.por

# We need to do some cleaning - G3 is the target (response)
# variable, and it will be dependent on G1 and G2
# (both unknown given the preconditions).
myData$G1=NULL
myData$G2=NULL
# myData = as.data.frame(unclass(myData)) only needed if read in using read_csv
summary(myData)

# The 'paid' variable is probably not helpful, and we
# should include the 'health' value of the 'Fjob'
# variable in with 'other'. This will help when we do
# k-fold CV later.
myData$paid=NULL
levels(myData$Fjob)=c("at_home", "other", "other", "services", "teacher")
summary(myData$Fjob)

# First we will build a linear regression model
# and cross validate it.
# Attach the "boot" package
glm.fit = glm(G3~., data=myData)
cv.out = cv.glm(myData, glm.fit, K=10)
cv.out$delta[1]
summary(myData$G3)

# Next we will try Ridge and Lasso. Note we use the built
# in CV function to select the best lambda, and this will
# also give the test MSE error estimate for that lambda.
# Attach the glmnet package first.
# Recall we need to use the predictor and response variables
# as input to create the model. We will again test a wide
# range of lambdas to find the best one.
x=model.matrix(G3~.,myData)[,-1]
y=myData$G3
cv.out=cv.glmnet(x,y,alpha=0)
names(cv.out)

# Lambda.min will give us the best lambda in this grid,
# and cvm will give us the MSE for that lambda (10-fold
# is the default approximation method).
cv.out$lambda.min
cv.out$cvm
min(cv.out$cvm)

# Very much in line with the linear regression model.
# Let's try Lasso
cv.out=cv.glmnet(x,y,alpha=1)
min(cv.out$cvm)

# Slightly worse. Note the cv method tested a default range of
# lambdas - we did not use the grid as before.
?cv.glmnet
dim(x)
bestlam =cv.out$lambda.min
bestlam
lasso.coef=predict(cv.out,type ="coefficients",s=bestlam )[1:37,]
lasso.coef[lasso.coef!=0]

# Now let's try PCR. Attach the "pls" package.
pcr.fit = pcr(G3~., data=myData, scale=TRUE, validation="CV")

# The option 'validation' allows us to view CV results on the
# different numbers of components
validationplot(pcr.fit, val.type="MSEP")
?pcr
summary(pcr.fit)

# It appears we can get an MSE comparable to Linear Regression
# using 19 components - not much of an advantage.
# Now let's try KNN. Attach the "FNN" package.
# Let's first do a simple CV. We select a set of training
# rows to use.
train = sample(1:649,500)
train.x = scale(x[train,])
test.x = scale(x[-train,])
train.y = y[train]
test.y = y[-train]
knn.fit = knn.reg(train.x, test.x, train.y, k=5)
mean((test.y - knn.fit$pred)^2)

# Somewhat worse, but K was arbitrary and this is simple CV.
# Let's first see if we can find a better k
errs = rep(0,15)
for(i in 1:15){
knn.fit = knn.reg(train.x, test.x, train.y, k=i)
errs[i] = mean((test.y - knn.fit$pred)^2)
}
errs

# What about doing k-fold CV?
bins = sample(1:10,649, replace = TRUE)

# Note this vector will assign every row in the data set to a bin
binErrs = rep(0,10)
for(k in 1:10){
train.x = scale(x[bins != k,])
test.x = scale(x[bins == k,])
train.y = y[bins != k]
test.y = y[bins == k]
knn.fit = knn.reg(train.x, test.x, train.y, k=8)
binErrs[k] = mean((test.y - knn.fit$pred)^2)
}
mean(binErrs)

# Let's combine...
errs = rep(0,15)
for(i in 1:15){
  for(k in 1:10){
  train.x = scale(x[bins != k,])
  test.x = scale(x[bins == k,])
  train.y = y[bins != k]
  test.y = y[bins == k]
  knn.fit = knn.reg(train.x, test.x, train.y, k=i)
  binErrs[k] = mean((test.y - knn.fit$pred)^2)
  }
  errs[i] = mean(binErrs)
}
errs

# Better than simple CV, but it appears that Ridge gave the
# best answer.
