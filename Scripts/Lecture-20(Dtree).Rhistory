# For our first example we will use the 'bands' dataset
View(bands)
summary(bands)

# We need to do some cleaning on this data
myData = as.data.frame(unclass(bands))
summary(myData)
myData[myData == "?"]=NA
summary(myData)

# Many of the predictors seem useless and can probably
# be dropped for a first model effort. In particular:

myData$V1 = NULL
myData$V2 = NULL
myData$V3 = NULL
myData$V4 = NULL
myData$V6 = NULL
myData$V7 = NULL
myData$V8 = NULL
myData$V9 = NULL
myData$V12 = NULL
myData$V13 = NULL
myData$V18 = NULL

# We need to reset the levels on some of the variables
levels(myData$V10)=c("COATED","COATED","COATED","UNCOATED","UNCOATED")
levels(myData$V11)=c("COATED","COATED","COVER","COVER","UNCOATED","UNCOATED")

for(i in 10:28){
myData[,i] = as.numeric(as.character(myData[,i]))
}

# Several more variables are of questionable value - 
# leave out in this frist draft of a model

myData$V32 = NULL
myData$V33 = NULL
myData$V39 = NULL
myData$V19 = NULL

# Finally, there are a few low frequency values to be removed

myData = myData[myData$V11!= 'COVER',]
myData = myData[myData$V15!= 'Motter70',]
myData = na.omit(myData)

train = sample(342,250)

# Let's first try Logistic Regression
lr.bands = glm(V40~., data=myData, subset=train, family = binomial)
lr.pred = rep(0,92)
lr.probs = predict(lr.bands, myData[-train,], type="response")
lr.pred[lr.probs>.5]=1
table(lr.pred,myData[-train,]$V40)

# Next we'll try Bagging - let's see how many predictors we have
dim(myData)

# Attach the 'randomForest' library
bag.bands = randomForest(V40~., data=myData,subset=train,mtry=24,importance=TRUE)

# The predict function will give us the actual class prediction, 
# not probs
bag.bands.pred = predict(bag.bands,myData[-train,],type="class")
table(bag.bands.pred,myData[-train,]$V40)

# Comparable to LR. Let's try a Random Forest with 6 predictors
rf.bands = randomForest(V40~., data=myData,subset=train,mtry=6,importance=TRUE)
rf.bands.pred = predict(rf.bands,myData[-train,],type="class")
table(rf.bands.pred,myData[-train,]$V40)

# Try to build a classifier for the wine quality datasets ...





