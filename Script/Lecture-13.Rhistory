# Read in the "spiritwt" dataset
colnames(spiritwt_data)=c("Temp","Dilution","Wght")
lr.model = lm(Wght~Temp+Dilution, data=spiritwt_data)
summary(lr.model)
dim(spiritwt_data)

# Here we select the rows that we will use to train the model
train = sample(315,215)
lr.model.1 = lm(Wght~Temp+Dilution, data=spiritwt_data, subset=train)

# R automatically uses only the rows in the train subset to
# create the model
summary(lr.model.1)

# Still looks pretty good ...
# Here is how we can calculate the training MSE
mean(lr.model.1$residuals^2)

# Is this good?
summary(spiritwt_data$Wght)
sqrt(mean(lr.model.1$residuals^2))/mean(spiritwt_data$Wght)

# The SQRT(MSE) is about 0.7% of the average WGHT, so
# probably a good result. But what about the Test MSE?
attach((spiritwt_data))
mean((Wght - predict(lr.model.1,spiritwt_data))[-train]^2)
sqrt(mean((Wght - predict(lr.model.1,spiritwt_data))[-train]^2))/mean(spiritwt_data$Wght)

# Pretty good so far - but this is just one Test set.
# To do LOOCV, we need to attach the 'boot' package
lr.model.2 = glm(Wght~Dilution+Temp, data=spiritwt_data)
summary(lr.model.2)

# Note this gives the same model (coefficients are the same)
# But the summary provides different information
# This does LOOCV ...
cv.error.2 = cv.glm(spiritwt_data,lr.model.2)# get Cv .one  test rest all traiining
cv.error.2$delta

# In line with the Test MSE we saw earlier ...
?cv.glm
plot(lr.model.2$residuals~ Dilution)

# Definitely looks like a nonlinear effect - what order
# poly to use? Let's test several ...
cv.error = rep(0,8)
for(i in 1:8){
glm.fit=glm(Wght~Temp+poly(Dilution,i), data=spiritwt_data)
cv.error[i]=cv.glm(spiritwt_data,glm.fit)$delta[1]
}
plot(cv.error)

# Looks like a quadratic gives the best result
cv.error

# The plot is deceptive - the best ploynomial is actually
# higher order - but quadratic is pretty good. Could go with
# quadratic or cubic here.

# Next read in the "airfoil" dataset
summary(airfoil_self_noise)
plot(airfoil_self_noise)

# Let's try the above method to see what level of polynomial
# works ...
cv.error.airfoil = rep(0,5)
attach(airfoil_self_noise)
for(i in 1:5){
glm.fit = glm(SoundLev~poly(Freq,i)+poly(Angle,i)+ChordLen + Velocity+poly(Displace,i),data=airfoil_self_noise)
cv.error.airfoil[i] = cv.glm(airfoil_self_noise,glm.fit,K=10)$delta[1]
}
cv.error.airfoil

# We applied 10-fold CV on each model to compare the MSEs ...
# There does not seem to be  big difference
# Let's look at the linear model
glm.fit = glm(SoundLev~Freq+Angle+ChordLen+Velocity+Displace, data=airfoil_self_noise)
summary(glm.fit)
plot(glm.fit$residuals~glm.fit$fitted.values)
cv.out=cv.glm(airfoil_self_noise,glm.fit,K=10)
cv.out$delta[1]
summary(airfoil_self_noise$SoundLev)

# Our SQRT(MSE) is about 3.8% of the mean value
# Let's try adding some cross terms
glm.fit = glm(SoundLev~Freq+Angle+ChordLen+Velocity+Displace+Freq*Angle+Angle*Displace+Freq*Displace, data=airfoil_self_noise)
summary(glm.fit)
glm.fit = glm(SoundLev~Freq+Angle+ChordLen+Velocity+Displace+Freq*Angle, data=airfoil_self_noise)
cv.out = cv.glm(airfoil_self_noise,glm.fit,K=10)
cv.out$delta[1]

# Slightly better ...
