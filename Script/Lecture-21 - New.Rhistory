# First read in the gardasil dataset - note this is 
# Excel format


lr.probs = predict(lr.model, newdata = gardasil[-train,], type="response")
lr.pred = rep(0,413)
lr.pred[lr.probs>.5]=1
table(lr.pred,gardasil[-train,]$Completed)

# Almost perfect - note number of shots should correlate almost
# perfectly with completion!
plot(gardasil$Completed~gardasil$Shots)
myData=gardasil
myData$Shots=NULL
lr.model = glm(Completed~., data=myData, subset=train, family = binomial)
lr.probs = predict(lr.model, newdata = myData[-train,], type="response")
lr.pred = rep(0,413)
lr.pred[lr.probs>.5]=1
table(lr.pred,myData[-train,]$Completed)

# Not great - missed most of the people who completed
lr.pred = rep(0,413)
lr.pred[lr.probs>.333]=1
table(lr.pred,myData[-train,]$Completed)

# Better. Let's try a RF ... attach the randomForest package
rf.model = randomForest(Completed~., data=myData,subset=train,mtry=3,importance=TRUE)

# RF expects the response to be a factor ...
myData_2 = myData
myData_2$Completed=as.factor(myData_2$Completed)
rf.model = randomForest(Completed~., data=myData_2,subset=train,mtry=3,importance=TRUE)
rf.pred = predict(rf.model,myData_2[-train,],type="class")
importance(rf.model)
table(rf.pred,myData_2[-train,]$Completed)

# Still not great. Next we will try a SVM. Attach the 'e1071'
# package. We first specify a linear kernel ...
?svm

# We will specify a cost C=1 to start ...
svm.lin = svm(Completed~., data=myData_2[train,], kernel = "linear", cost=1, scale=FALSE)
svm.lin.pred = predict(svm.lin,myData_2[-train,])
table(svm.lin.pred,myData_2[-train,]$Completed)

# Just predict no one finishes!
mean(svm.lin.pred == myData_2[-train,]$Completed)

# We have a wide range of values, let's try scaling ...
svm.lin = svm(Completed~., data=myData_2[train,], kernel = "linear", cost=1, scale=TRUE)
svm.lin.pred = predict(svm.lin,myData_2[-train,])
table(svm.lin.pred,myData_2[-train,]$Completed)

# No help ...
# e1071 has a built in tuning function which uses CV to find the 
# best value for C ...
tune.lin = tune(svm,Completed~., data=myData_2[-train,], kernel="linear", ranges=list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(tune.lin)

# Strange that the different C values had no impact ...
bestmod.lin = tune.lin$best.model
table(predict(bestmod.lin,myData_2[-train,]),myData_2[-train,]$Completed)

# The linear model is still predicting all '0'
# Let's try a nonlinear kernel ...
svm.nonlin_p = svm(Completed~.,data=myData_2[train,],kernel="polynomial", gamma=1, cost=1)
table(predict(svm.nonlin_p,myData_2[-train,]),myData_2[-train,]$Completed)

# Now predicting some '1', but not well
# We can also try to tune parameters for the polynomial
# kernel if we want
svm.radial = svm(Completed~.,data=myData_2[train,],kernel="radial", gamma=1, cost=1)
table(predict(svm.radial,myData_2[-train,]),myData_2[-train,]$Completed)

# Let's try another dataset - read in the Parkinsons data
summary(parkinsons_csv)
myData=parkinsons_csv
myData$status=as.factor(myData$status)
summary(myData)
myData$name=NULL
train=sample(195,150)
svm.lin = svm(status~., data=myData[train,], kernel = "linear", cost=1, scale=TRUE)
svm.lin.pred = predict(svm.lin,myData[-train,])
table(svm.lin.pred,myData[-train,]$status)

# Seems to work well ... let's try some nonlinear models
svm.nonlin_p = svm(status~.,data=myData[train,],kernel="polynomial", gamma=1, cost=1, scale = TRUE)
svm.nonlin_p.pred = predict(svm.nonlin_p,myData[-train,])
table(svm.nonlin_p.pred,myData[-train,]$status)

# Better!
svm.nonlin_p = svm(status~.,data=myData[train,],kernel="radial", gamma=1, cost=1, scale = TRUE)
svm.nonlin_p.pred = predict(svm.nonlin_p,myData[-train,])
table(svm.nonlin_p.pred,myData[-train,]$status)

# Not as good.
# Looks like a polynomial kernel worked best ... can further
# tune the parameters.

# Attach the mlBench package - it contains several datasets

library(help="mlbench")

# Let's try SVM on a couple of sets

data("BreastCancer")
myData = BreastCancer
summary(myData)
myData = na.omit(myData)
dim(myData)
myData$Id=NULL

# We will do a linear SVM, and try to tune the cost

tune.lin = tune(svm,Class~., data=myData, kernel="linear", ranges=list(cost = c(.001, .01, .1, 1, 5, 10, 100)))

summary(tune.lin)

# Looking at the summary, we can see a good error rate for
# cost = 0.1, using 10-fold CV.
# Next let's try Sonar ...

data(Sonar)
myData = Sonar
summary(Sonar)
tune.lin = tune(svm,Class~., data=myData, kernel="linear", ranges=list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(tune.lin)

tune.lin = tune(svm,Class~., data=myData, kernel="polynomial", ranges=list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(tune.lin)

# Polynomial seems better - let's try radial

tune.lin = tune(svm,Class~., data=myData, kernel="radial", ranges=list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(tune.lin)
tune.lin = tune(svm,Class~., data=myData, kernel="radial", cost = 100, ranges=list(gamma = c(.001, .01, .1, 1, 5, 10, 100)))
summary(tune.lin)
tune.lin = tune(svm,Class~., data=myData, kernel="radial", gamma= .01, ranges=list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(tune.lin)


