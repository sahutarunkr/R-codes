# We will use a data set from the ISLR package
# First attach "ISLR"
fix(Hitters)

# Fix gives a pop-up editor for editing data files (be careful
# with big ones)
names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))

# This tells us 59 records have NAs, so we are safe removing them
Hitters=na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters))
set.seed(1)

# We will use the "glmnet" package
# We need to feed glmnet the predictors and response variable
# separately
x=model.matrix(Salary~.,Hitters)[,-1]
y=Hitters$Salary

# We create a sequence of lambdas to test
grid=10^seq(10,-2,length=100)

# We now do Ridge reqression for each lambda. Be sure attach the
# "glmnet" package first
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
dim(coef(ridge.mod))

# Notice we set alpha = 0; this is the option for Ridge
# Also, the glmnet scales the data automatically
# The coef matrix has a row for each predictor (plus intercept)
# and col for each lambda value
names(ridge.mod)

# We can access the lambda values, but the coeff we need to
# access separately
ridge.mod$lambda [50]
coef(ridge.mod)[,50]

# The predict function allows us to calculate the coefficients
# for lambdas that were not in our original grid
# Here are the coefficients for lambda = 50 ...
predict(ridge.mod,s=50,type="coefficients")[1:20,]

# We would like to choose an optimal lambda - glmnet allows us
# to automatically do 10-fold CV for each lambda and then
# pick the lowest value ...
set.seed(1)
cv.out=cv.glmnet(x,y,alpha=0)

# We can find the best lambda and best MSE as follows ...
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam
min(cv.out$cvm)

# Note in the previous example all coefficients are kept,
# but some are weighted more than others.

# Now we apply the Lasso - note all we do is change the alpha
# option
lasso.mod =glmnet (x,y,alpha =1, lambda =grid)
plot(lasso.mod)

# Note that Lasso takes certain coeff to zero for large enough
# lambda.We'll do CV to find the best lambda again ...
set.seed (1)
cv.out =cv.glmnet (x,y,alpha =1)
plot(cv.out)
bestlam =cv.out$lambda.min
bestlam
min(cv.out$cvm)

# The MSE is better compared to what we saw for Ridge
# Let's find the coefficients ...
lasso.coef=predict(lass.mod,type ="coefficients",s=bestlam )[1:20,]
lasso.coef
lasso.coef[lasso.coef!=0]

# Read in the Parkinsons dataset
View(parkinsons_updrs.data)
summary(parkinsons_updrs.data)
myData = parkinsons_updrs.data

# We will eliminate some variables
myData$subject=NULL
myData$test_time=NULL

# We will model the total UPDRS first - note
# we remove the motor score
x=model.matrix(total_UPDRS~.-motor_UPDRS,myData)[,-1]
y=myData$total_UPDRS
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
cv.out=cv.glmnet(x,y,alpha=0)
plot(cv.out)
min(cv.out$cvm)
summary(myData$total_UPDRS)

# Let's try Lasso ...
lasso.mod =glmnet (x,y,alpha =1, lambda =grid)
plot(lasso.mod)
cv.out =cv.glmnet (x,y,alpha =1)
plot(cv.out)
min(cv.out$cvm)

# Now we try to model the motor UPDRS ...
x=model.matrix(motor_UPDRS~.-total_UPDRS,myData)[,-1]
y=myData$motor_UPDRS
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
cv.out=cv.glmnet(x,y,alpha=0)
plot(cv.out)
min(cv.out$cvm)
summary(myData$motor_UPDRS)

# Let's try Lasso ...
lasso.mod =glmnet (x,y,alpha =1, lambda =grid)
plot(lasso.mod)
cv.out =cv.glmnet (x,y,alpha =1)
plot(cv.out)
min(cv.out$cvm)

